# -*- coding: utf-8 -*-
"""Sentiment Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N0TqbfGlZec0Ko5igrqPe20aSl2ubRDu

## Importing data and fixing column names and types

### Importing dataset from kaggle
"""

import pandas as pd
import numpy as np

df = pd.read_csv("/content/sample_data/train.csv",encoding='unicode_escape', usecols=['text', 'sentiment'])
df.rename(columns={'text': 'comment'}, inplace=True)

df

df = df.dropna()

df.loc[:, 'sentiment'] = df['sentiment'].map({"positive":1, "negative":-1, "neutral":0})

df

"""## Train, Test, Split"""

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(df[['comment']], df[['sentiment']], random_state=25, test_size=0.2)

x_train

x_test

"""### Making dataframe insted of printing

## Text Preprocessing

### Lower Casing
"""

x_train['comment'][391].lower()

x_train['comment'].str.lower()

x_train['comment'] = x_train['comment'].str.lower()
x_test['comment'] = x_test['comment'].str.lower()

"""### Convert html unescape characters"""

text1 = "Didn&#39;t catch it earlier but man at start of video said Punkey Brewster. Now that is some 80s shit, when I grew up. Felt good hearing that as I missed it in earlier viewings of this video"
text2 = "so this is great! one thing I am missing is why keep using &#39;bro&#39; &amp; &#39;brother&#39;  just does not fit!!"
text3 = "Action after sipping the wine &quot;it&#39;s gentle like a kiss, no tongue&quot;"
text4 = "Part I &quot;Le Retour&quot;, no S, shall ask for correction!"
text5 = "I&#39;M WATCHING THIS WHILE I&#39;M EATING.. I&#39;M STILL HUNGRY"

import html

def convert_html_unescapes(text):
  return html.unescape(text)

print(convert_html_unescapes(text1))
print(convert_html_unescapes(text2))
print(convert_html_unescapes(text3))
print(convert_html_unescapes(text4))
print(convert_html_unescapes(text5))

x_train['comment'].apply(convert_html_unescapes)

x_train['comment'] = x_train['comment'].apply(convert_html_unescapes)
x_test['comment'] = x_test['comment'].apply(convert_html_unescapes)

"""### HTML tags removal"""

x_train['comment'][5]

import re
def remove_html_tags(text):
  pattern = re.compile('<.*?>')
  return pattern.sub(r' ',text)

text = '<html> hdsahjd</html>quoi sevin!?<br>le poulet j&#39;ai criend! hereðŸ˜‚ <a href="https://www.youtube.com/watch?v=-GJgqIJsTME&amp;t=98">1:38</a>. Great episode!'

remove_html_tags(text)

x_train['comment'].apply(remove_html_tags)

x_train['comment'] = x_train['comment'].apply(remove_html_tags)

x_test['comment'] = x_test['comment'].apply(remove_html_tags)

"""### Remove URLs"""

text1 = 'Check out our e-shop http://www.google.com/kaggle fhsdjkfgs'
text2 = 'Check out our e-shop http://www.google.com ghf'
text3 = 'Check out our e-shop www.google.com/kaggle'
text4 = 'Check out our e-shop https://www.google.com'
text5 = 'Check out our e-shop https://www.google.com/kaggle/ gfgfg'
text6 = 'Check out our e-shop http://www.google.com/kaggle/'

text4

def remove_URLs(text):
  pattern = re.compile(r'https?://\S+|www\.\S+')
  return pattern.sub(r' ', text)

print(remove_URLs(text1))
print(remove_URLs(text2))
print(remove_URLs(text3))
print(remove_URLs(text4))
print(remove_URLs(text5))
print(remove_URLs(text6))

x_train['comment'] = x_train['comment'].apply(remove_URLs)
x_test['comment'] = x_test['comment'].apply(remove_URLs)

"""### Replace ` to '"""

x_train.loc[:,'comment'] = x_train['comment'].str.replace('`', "'")
x_test.loc[:,'comment'] = x_test['comment'].str.replace('`', "'")

"""### Remove chat words"""

chat_words = {'AFAIK':'As Far As I Know',
'AFK':'Away From Keyboard',
'ASAP':'As Soon As Possible',
'ATK':'At The Keyboard',
'ATM':'At The Moment',
'A3':'Anytime, Anywhere, Anyplace',
'BAK':'Back At Keyboard',
'BBL':'Be Back Later',
'BBS':'Be Back Soon',
'BFN':'Bye For Now',
'B4N':'Bye For Now',
'BRB':'Be Right Back',
'BRT':'Be Right There',
'BTW':'By The Way',
'B4':'Before',
'B4N':'Bye For Now',
'CU':'See You',
'CUL8R':'See You Later',
'CYA':'See You',
'FAQ':'Frequently Asked Questions',
'FC':'Fingers Crossed',
'FWIW':"For What It's Worth",
'FYI':'For Your Information',
'GAL':'Get A Life',
'GG':'Good Game',
'GN':'Good Night',
'GMTA':'Great Minds Think Alike',
'GR8':'Great!',
'G9':'Genius',
'IC':'I See',
'ICQ':'I Seek you (also a chat program)',
'ILU':'I Love You',
'IMHO':'In My Humble Opinion',
'IMO':'In My Opinion',
'IOW':'In Other Words',
'IRL':'In Real Life',
'KISS':'Keep It Simple, Stupid',
'LDR':'Long Distance Relationship',
'LMAO':'Laugh My A.. Off',
'LOL':'Laughing Out Loud',
'LTNS':'Long Time No See',
'L8R':'Later',
'MTE':'My Thoughts Exactly',
'M8':'Mate',
'NRN':'No Reply Necessary',
'OIC':'Oh I See',
'PITA':'Pain In The Ass',
'PRT':'Party',
'PRW':'Parents Are Watching',
'QPSA?':'Que Pasa?',
'ROFL':'Rolling On The Floor Laughing',
'ROFLOL':'Rolling On The Floor Laughing Out Loud',
'ROTFLMAO':'Rolling On The Floor Laughing My Ass Off',
'SK8':'Skate',
'STATS':'Your sex and age',
'ASL':'Age, Sex, Location',
'THX':'Thank You',
'TTFN':'Ta-Ta For Now!',
'TTYL':'Talk To You Later',
'U':'You',
'U2':'You Too',
'U4E':'Yours For Ever',
'WB':'Welcome Back',
'WTF':'What The Fuck',
'WTG':'Way To Go!',
'WUF':'Where Are You From?',
'W8':'Wait...',
'7K':'Sick Laugher',
'TFW':'That feeling when. TFW internet slang often goes in a caption to an image.',
'MFW':'My face when',
'MRW':'My reaction when',
'IFYP':'I feel your pain',
'LOL':'Laughing out loud',
'TNTL':'Trying not to laugh',
'JK':'Just kidding',
'IDC':"I don't care",
'ILY':'I love you',
'IMU':'I miss you',
'ADIH':'Another day in hell',
'IDC':"I don't care",
'ZZZ':'Sleeping, bored, tired',
'WYWH':'Wish you were here',
'TIME':'Tears in my eyes',
'BAE':'Before anyone else',
'FIMH':'Forever in my heart',
'BSAAW':'Big smile and a wink',
'BWL':'Bursting with laughter',
'LMAO':'Laughing my ass off',
'BFF': 'Best friends forever',
'CSL':"Can't stop laughing"}

def chat_conversion(text):
  new_text = []
  for w in text.split():
    if w.upper() in chat_words:
      new_text.append(chat_words[w.upper()].lower())
    else:
      new_text.append(w)
  return " ".join(new_text)

text = "i need that thing asap"
print(chat_conversion(text))
text = "i bored : hope your wlan will go today!"
print(chat_conversion(text))

x_train['comment'] = x_train['comment'].apply(chat_conversion)
x_test['comment'] = x_test['comment'].apply(chat_conversion)

x_train

"""### Stop words removal"""

import nltk

# download once
# nltk.download('stopwords')
# nltk.download('punkt')

from nltk.corpus import stopwords

st = stopwords.words('english')
st.extend(["i'm"])

sorted(st, key=str.lower)

def remove_stopwords(text):
  new_text = []

  for word in text.split():
    if word in st:
      new_text.append('')
    else:
      new_text.append(word)
  x = new_text[:]
  new_text.clear()
  return " ".join(x)

remove_stopwords("Man, don't  that wine looks so nice. The red wines Clovis always brings out isn&#39;t like the normal red wine color. It always looks like a fruit punch strawberry lemonade color and looks so delicious.")

x_train['comment'][26768]

x_train['comment'].apply(remove_stopwords)

x_train['comment'] = x_train['comment'].apply(remove_stopwords)
x_test['comment'] = x_test['comment'].apply(remove_stopwords)

"""### Remove Punctuation
It should be called after all preprocessing
"""

import string

exclude = string.punctuation
exclude

def remove_punc(text):
  return text.translate(str.maketrans('','',exclude))

print(remove_punc("cdsdcsd'sdsd?-bbn.>()"))

x_train['comment'].apply(remove_punc)

x_train['comment'] = x_train['comment'].apply(remove_punc)
x_test['comment'] = x_test['comment'].apply(remove_punc)

"""### Handling emojis"""

print(df['comment'][249])
print(df['comment'][370])

!pip install emoji

import emoji

print(emoji.demojize(df['comment'][249]))
print(emoji.demojize(df['comment'][370]))

emoji.demojize("sdgsadhfdhg ðŸ”¥")

df['comment'].apply(emoji.demojize)[249]

x_train['comment'] = x_train['comment'].apply(emoji.demojize)
x_test['comment'] = x_test['comment'].apply(emoji.demojize)

x_train['comment']

"""### Stemming"""

from nltk.stem.porter import PorterStemmer

ps = PorterStemmer()
def stem_words(text):
  return " ".join([ps.stem(word) for word in text.split()])

text = "walk walked walking walks"
stem_words(text)

text = "act acts actor acting actress"
stem_words(text)

df['comment'][2]

# sometime word cannot be a valid word in the languag as well, if we want to that it must be valid word in language, then, we must use lemmatization(slow in running) which ensures that the word is a valid word
stem_words(df['comment'][2])

x_train['comment'].apply(stem_words)

x_train['comment'] = x_train['comment'].apply(stem_words)
x_test['comment'] = x_test['comment'].apply(stem_words)

"""### Tokenization"""

text1 = "Man, that wine looks so nice. The red wines Clovis always brings out isn&#39;t like the normal red wine color. It always looks like a fruit punch strawberry lemonade color and looks so delicious."
text2 = """Didn't catch it earlier but man at start of video said Punkey Brewster. Now that is some 80s shit, when I grew up. Felt good hearing that as I missed it in earlier viewings of this video
so this is great! one thing I am missing is why keep using just does not fit!!
I watched this whole series backwards lol"""
text3 = 'I have a PH.D in A.I.'

import spacy

tokenise = spacy.load('en_core_web_sm')

doc1 = tokenise(text1)
doc2 = tokenise(text2)
doc3 = tokenise(text3)

for token in doc1:
  print(token)

for token in doc2:
  print(token)

for token in doc3:
  print(token)

import gensim

from nltk import sent_tokenize
from gensim.utils import simple_preprocess

corpus = []
def make_tokenised_2d_array(text):
  raw_sent = sent_tokenize(text)
  for sent in raw_sent:
    corpus.append(simple_preprocess(sent))

x_train['comment'].apply(make_tokenised_2d_array)

corpus

model = gensim.models.Word2Vec(window=10, min_count=2, workers=4)

model.build_vocab(corpus)

model.train(corpus, total_examples=model.corpus_count, epochs=model.epochs)

model.wv.most_similar('thank')

model.wv.most_similar('hi')

model.wv.similarity('hi','hey')

model.wv.index_to_key









# import googleapiclient.discovery
# import googleapiclient.errors

# api_service_name = "youtube"
# api_version = "v3"
# DEVELOPER_KEY = "AIzaSyDK_z4DwCkXkpieD9UyWlaQyo4vxE8w57I"

# youtube = googleapiclient.discovery.build(
#     api_service_name, api_version, developerKey=DEVELOPER_KEY
# )

# request = youtube.commentThreads().list(
#     part="snippet",
#     videoId="-GJgqIJsTME",
#     maxResults=100
# )

# response = request.execute()

# response['items'][0]

# for item in response['items']:
#   print(item['snippet']['topLevelComment']['snippet']['textDisplay'])

# comment_list = []
# for item in response['items']:
#   comment = item['snippet']['topLevelComment']['snippet']
#   comment_list.append([
#       comment['textDisplay'],
#       comment['likeCount']
#   ])

# df = pd.DataFrame(comment_list, columns=['comment', 'likeCount'])

# df.head()

# df

# while(1 == 1):
#   try:
#     nextPageToken = response['nextPageToken']
#   except KeyError:
#     break
#   nextPageToken = response['nextPageToken']

#   nextRequest = youtube.commentThreads().list(
#       part="snippet",
#       videoId="-GJgqIJsTME",
#       maxResults=100,
#       pageToken=nextPageToken
#   )
#   response = nextRequest.execute()

#   for item in response['items']:
#     comment = item['snippet']['topLevelComment']['snippet']
#     comment_list.append([
#         comment['textDisplay'],
#         comment['likeCount']
#     ])
# df = pd.DataFrame(comment_list, columns=['comment', 'likeCount'])

# df

# df.head()

# df.info()

# df.sample(5)







"""### Spelling correction"""

# from textblob import TextBlob

# def spell_correction(text):
#   textBlb = TextBlob(text)
#   return textBlb.correct().string

# text = "ceertain connditionas duriing serveal ggenerations aree moodified in the saame maner"
# spell_correction(text)

"""### Calm down!!! It will take time. Sip some water."""

# from concurrent.futures import ThreadPoolExecutor

# # Function to apply in parallel
# def parallel_apply(df_column, func):
#     with ThreadPoolExecutor() as executor:
#         results = list(executor.map(func, df_column))
#     return results

# parallel_apply(x_train['comment'], spell_correction)

# x_train['comment'] = x_train['comment'].apply(spell_correction)
# x_test['comment'] = x_test['comment'].apply(spell_correction)